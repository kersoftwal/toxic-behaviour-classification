{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a20ea0",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581021a0",
   "metadata": {},
   "source": [
    "Online conversations in big communities allow people share ideas and help each other, but when users are having a talk and rude and disrespectful comments are found within the lines, people tend to either abandon the discussion or, in the worst case, feed that behavior. In order to avoid this kind of situations and make big communities a better place to stay and grow, it is good to be able to recognize toxic behaviour and address it consequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13026822",
   "metadata": {},
   "source": [
    "For this purpose, we are using a dataset for toxic comment classification provided in a competition shape on kaggle by the Conversation AI team Google: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge. It is posed as a multilabel classification challenge in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf710cd",
   "metadata": {},
   "source": [
    "The purpose of this document is to go through the decision-making process in a plan-test-observe cycle fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a93163",
   "metadata": {},
   "source": [
    "Code, docs and experiments sheet can be found at: https://github.com/kersoftwal/toxic-behaviour-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70224848",
   "metadata": {},
   "source": [
    "**DATA ANALYSIS** revealed that the dataset contains about 160.000 rows short conversations, which we want to label with several kind of toxicity, namely: toxic, severe_toxic, obscene, threat, insult, identity_hate. The distribution of labels is heavily imbalanced in the dataset, among which 88% of these rows have no toxic behaviour, while the remaining 12% have between 4% and 0.02% percentage of appearance in the dataset [for more details, refer to the data_analysis.ipynb document]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186b82e",
   "metadata": {},
   "source": [
    "**PREPROCESSING** featured several potential operations to apply on the dataset to improve the results, among which we remember:\n",
    "* lowering of the words (we've seen that the uppercasing is not correlated with any of the interested classes, so we're not losing info with this prep step)\n",
    "* abbreviations removal (sometimes abbreviations are used in textual conversations, we will try to address the most common abbreviations)\n",
    "* punctuation removal (not really needed as soon as we're considering words as features)\n",
    "* number removal (same as punctuation removal)\n",
    "* tokenization \n",
    "* stopwords removal (no semantic usefulness for the toxicity detection task)\n",
    "* lemmatization (to remove inflection and increase the amount of shared words between conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb0255",
   "metadata": {},
   "source": [
    "These preprocessing steps were tested on a logistic regression model (adopted as a base model) [further details on the experiments.ods document]. The **ENGINEERED FEATURES** to be extracted from text were word counts and tf-idf. Several experiments were carried out and overall the tf-idf features revealed to be more impactful than simple word counts. Moreover not all the preprocessing steps provided improvements on the validation data: the ones from which we could retain more value was:\n",
    "* for word counts: word lowering, punctuation removal and stopwords removal.\n",
    "* for word tf-idf: word lowering, abbreviations removal and number removal.\n",
    "\n",
    "Hence we retained the ones for tf-idf features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd057b",
   "metadata": {},
   "source": [
    "The official metric considered by the kaggle challenge is the accuracy, still I decided to look at different **METRICS** that better suit an heavily imbalanced labels distribution, namely the confusion matrix for each label, the f1-score for each label, the micro-f1 score. It was decided not to go with the weighted-f1 score because I wanted to give the same importance to each label even though they have a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381bf501",
   "metadata": {},
   "source": [
    "For what concerns experimentation, the training dataset was split into training and validation set with a 70-30 split. The test set has not been considered in the experiments yet.\n",
    "\n",
    "Several **MODELS** were tried by tuning them and trying to get the best out of each model:\n",
    "* Logistic regression proved to be a very solid baseline whatsoever. \n",
    "* SVC Linear got similar results to logistic regression, slightly better but more computational expensive, so it is less preferred here.\n",
    "* Randomforests can obtain similar (somewhat worse) results to logistic regression, still probably overfitting because it manages to get those with a really high tree depth, so this was not a reasonable choice. \n",
    "* Gradient Boosted classifiers achieve remarkably better results with a low tree depth, even achieve further accurate results (maybe overfitting?). \n",
    "\n",
    "Every model was saved with a json holding the results. (time to try and see what can be obtained with the test set?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f436e",
   "metadata": {},
   "source": [
    "Relatively to the **IMBALANCE** in the dataset, it seems to be really difficult to deal with since it is a multilabel classification, i.e. more than one label can be assigned to each row and we have so little of toxic rows. In order to address this, Label Powerset was considered, but this leads to even less rows: might be ideal when the dataset is slightly imbalanced, not in this case, in fact much poor results were yielded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe15ad3",
   "metadata": {},
   "source": [
    "### Current things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b09937",
   "metadata": {},
   "source": [
    "So far we have good baselines to compare other models with. \n",
    "\n",
    "1) as a new feature to exploit it could be interesting to use learnt word embeddings.\n",
    "\n",
    "2) logistic regression proved to be a solid baseline, maybe a shallow neural network can help in pushing the f1 numbers for each label. Among the ideas, there is autoencoders and typical RNNs (maybe with skip connections).\n",
    "\n",
    "No ideas on how to deal with imbalance so far..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630604fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
